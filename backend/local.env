# GEMINI_API_KEY= # No longer primary, can be removed or kept if you plan to use Google Search tools

OPENAI_API_BASE="http://localhost:1234/v1"
OPENAI_API_KEY="not_needed" # Or your LM Studio API key if you've configured one

# Option 1: Use a single local model for all tasks
# LOCAL_MODEL_NAME="your-lmstudio-model-identifier" # e.g., TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf

# Option 2: Specify different local models (ensure these match models loaded in LM Studio)
# Ensure these are set in your actual .env file
# QUERY_GENERATOR_MODEL="your-lmstudio-model-for-queries"
# REFLECTION_MODEL="your-lmstudio-model-for-reflection"
# ANSWER_MODEL="your-lmstudio-model-for-answers"

# If you still want to use Google Search with LangChain tools (not genai_client), you might need:
# GOOGLE_API_KEY="your_google_api_key_for_search"
# GOOGLE_CSE_ID="your_google_custom_search_engine_id"